{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF240 - Machine Learning and Deep Learning\n",
    "\n",
    "## Assignment 1: K-Means, Naive Bayes, SVMs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives \n",
    "\n",
    "The objective of this assignment is to apply different classification algorithms for the application of handwritten digits recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0 - Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, you will experiment with the well-known MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load and normalize MNIST testing and training data. This dataset is made of 60000 grayscale images of size 28$\\times$28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = MNISTtools.load(dataset='training')\n",
    "x_test, y_test = MNISTtools.load(dataset='testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the computation cost, we only select a small part of $n$ images for further analysis. _You might modify this value later to visualise its influence on the results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "x_train = x_train[:,1:n+1]\n",
    "y_train = y_train[1:n+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "How many training and testing images compose the dataset? What are the dimensions of the data samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 5000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the images, the vectors must be reshaped to a grayscale square image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label of image 42 is 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label of image 200 is 1\n"
     ]
    }
   ],
   "source": [
    "x_test_im = x_test.reshape((28, 28, 1, -1))\n",
    "x_train_im = x_train.reshape((28, 28, 1, -1))\n",
    "\n",
    "# let us show image number 42 and its label\n",
    "MNISTtools.show(x_train_im[:, :, 0, 42])\n",
    "print(\"the label of image 42 is\", y_train[42])\n",
    "\n",
    "# let us show image number 200 and its label\n",
    "MNISTtools.show(x_train_im[:, :, 0, 200])\n",
    "print(\"the label of image 200 is\", y_train[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - K-Means\n",
    "\n",
    "In this first part you must group the testing set into 10 clusters using the K-Means algorithm. \n",
    "You must observe the clustering result by visualizing as images the computed centroids and by checking if each found cluster corresponds to one label from the ground truth given by `y_test`. In other words, check to which cluster is assigned each data point, as in a classification problem. Analyse the results. \n",
    "\n",
    "Be careful: the index of the clusters you find (named `pred_labels` here) does not mandatorily correspond to the digit value (initial label `y_test`). One way to find which label corresponds to each cluster is to visualise the centroid image, or to compute the mode of each class in the ground truth, for example using the following code:\n",
    "\n",
    "```\n",
    "from scipy.stats import mode\n",
    "real_pred_labels = np.zeros_like(pred_labels)\n",
    "real_pred_centers = np.zeros_like(pred_centers)\n",
    "for i in range(10):\n",
    "    indices = np.where(pred_labels == i)[0]\n",
    "    real_value = mode(y_test[indices])[0]\n",
    "    real_pred_labels[indices] = real_value\n",
    "    real_pred_centers[real_value] = pred_centers[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose data, necessary for K-Means, Naive Bayes and SVM with sklearn\n",
    "x_test_t =  np.transpose(x_test)\n",
    "x_train_t =  np.transpose(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Implement K-Means. Add comments to your code as necessary to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Naive Bayes\n",
    "Apply Naive Bayes classification to the digits classification problem. Train the model on the training dataset and evaluate on the testing set. Analyse your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Implement Naive Bayes. Add comments to your code as necessary to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Analyse the results obtained on the testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - SVM\n",
    "Apply SVM classification with linear and polynomial/gaussian kernels to the digits classification problem. Train the models on the training dataset and evaluate on the testing set. Analyse your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Implement SVM classification with linear and polynomial/gaussian kernels. Add comments to your code as necessary to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Analyse the results obtained on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4 - CNNs\n",
    "\n",
    "CNNs are specific artificial neural networks composed of *convolutional* layers, *maxpooling* operations, and\n",
    "*fully connected* layers.\n",
    "- Convolutional layers are like typical layers where the weight matrix has a specific structure that is relevant for signals and images.  \n",
    "They take as input $N$ images and produce as output $C$ images (called *feature maps* or *channels*).   \n",
    "They are parameterized by a collection of coeficients that defines a filter bank. Each filter performs a weighted average of its inputs within local sliding windows of size $K \\times K$  (pixels) where $K$ is a hyperparameter (a small odd number: 3, 5, 7, 9).  \n",
    "As for classical layers in neural networks, each feature map is next processed by an activation function such as  ReLU.  \n",
    "    \n",
    "- Maxpooling operations reduce the dimensions of the feature maps by picking the maximum value within local but non-overlapping sliding windows of size $L \\times L$ (pixels) where $L$ is another hyper-parameter (usually 2). Maxpooling does not introduce new parameters to be learned.  \n",
    "  \n",
    "- Fully connected layers are standard layers where the weight matrix does not have a specific structure: each of the $N$ output units is connected to each of the $M$ input units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "PyTorch expects that the input of a convolutional layer is stored in the following format:\n",
    "  $$\n",
    "  \\texttt{Batch size} \\times \\texttt{Number of input channels} \\times \\texttt{Image height} \\times \\texttt{Image width}\n",
    "  $$\n",
    "  \n",
    "The number of input channels in our case is 1 because MNIST is composed of grayscale images. It would have been 3 if the images were in RGB color.\n",
    "In deeper layers, the number of input channels will be the number of feature maps coming from the previous layer.\n",
    "\n",
    "Reorganise the tensors `x_train_im` and `x_test_im` accordingly.\n",
    "\n",
    "Hint: you can reshape them first with shape $\\texttt{(28, 28, 1, 60000)}$ and $\\texttt{(28, 28, 1, 10000)}$ respectively and then use `np.moveaxis`. Other solutions are also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE\n",
    "x_test_im = x_test.reshape((28, 28, 1, -1))\n",
    "x_train_im = x_train.reshape((28, 28, 1, -1))\n",
    "x_test_im = np.moveaxis(x_test_im,[2,3],[1,0])\n",
    "x_train_im = np.moveaxis(x_train_im,[2,3],[1,0])\n",
    "print(x_train_im.shape)\n",
    "print(x_test_im.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also normalize MNIST testing and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    return 2 * x.astype(np.float32) / 255. - 1\n",
    "x_train_im = normalize_MNIST_images(x_train_im)\n",
    "x_test_im = normalize_MNIST_images(x_test_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally wrap all the data into torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.from_numpy(x_train_im)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "x_test_torch = torch.from_numpy(x_test_im)\n",
    "y_test_torch = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Neural networks can be constructed using the `torch.nn` package, which relies on `autograd` differentiation tools.\n",
    "This package provides an implementation of CNNs as follows:\n",
    "\n",
    "* Convolutional layers can be created as `nn.Conv2d(N, C, K)`.\n",
    "  For input images of size $W \\times H$, without padding the output feature maps have size $[W-K+1] \\times [H-K+1]$.\n",
    "* In PyTorch, maxpooling is implemented like any other non-linear function (such as\n",
    "  `ReLU` or `softmax`).\n",
    "  For input images of size $W \\times H$, the output feature maps\n",
    "  have size $\\lceil W/L \\rceil \\times \\lceil H/L \\rceil$.\n",
    "* A fully connected layer can be created as `nn.Linear(M, N)`.\n",
    "\n",
    "Our LeNet network will be composed successively of\n",
    "   1. a convolutional layer (i) connecting the input image to 6\n",
    "    feature maps with $5 \\times 5$ convolutions ($K = 5$) and followed\n",
    "    by ReLU and maxpooling (ii) ($L=2$),\n",
    "   2. a convolutional layer (ii) connecting the 6 input channels to 16\n",
    "    output channels with $5 \\times 5$ convolutions and followed\n",
    "    by ReLU and maxpooling (iv) ($L=2$),\n",
    "   3. a fully-connected layer connecting $16$ feature maps\n",
    "    to $120$ output units and followed by ReLU,\n",
    "   4. a fully-connected layer connecting $120$ inputs\n",
    "    to $84$ output units and followed by ReLU,\n",
    "   5. a final linear layer connecting $84$ inputs\n",
    "    to $10$ linear outputs (one for each of our digits).\n",
    "\n",
    "Determine the size of the feature maps after each convolution and maxpooling operation i.e. at points (i)-(iv) processing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Analyse and complete the following code initializing our LeNet network. \n",
    "\n",
    "Note that you just have to define the forward function, and the backward function (where gradients are computed) will be automatically defined for you using `autograd`.\n",
    "You can use any of the Torch tensor operations in the forward function.\n",
    "For more details, please refer to https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# The neural networks class\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    # define our network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6 , 5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)# COMPLETE\n",
    "        self.fc1 = nn.Linear(256,120)# COMPLETE\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)# COMPLETE\n",
    "        \n",
    "    # define one forward pass through the network\n",
    "                             \n",
    "    def forward(self, x):                  \n",
    "        conv1 =   F.relu(self.conv1(x))\n",
    "        maxpool1 = F.max_pool2d(conv1, (2, 2))\n",
    "        conv2 =  F.relu(self.conv2(maxpool1)) # COMPLETE\n",
    "        maxpool2 = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) # COMPLETE\n",
    "        flattened = maxpool2.view(-1, self.num_flat_features(maxpool2))\n",
    "        dense1 = F.relu(self.fc1(flattened))  # COMPLETE\n",
    "        dense2 = F.relu(self.fc2(dense1))\n",
    "        output = self.fc3(dense2)\n",
    "        return output\n",
    "    \n",
    "    # helper function to understand the dimensions\n",
    "                             \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        return np.prod(size)\n",
    "\n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "Run the following and interpret the results. What are the learnable parameters?  Are gradients going to be tracked for all parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5]) True\n",
      "conv1.bias torch.Size([6]) True\n",
      "conv2.weight torch.Size([16, 6, 5, 5]) True\n",
      "conv2.bias torch.Size([16]) True\n",
      "fc1.weight torch.Size([120, 256]) True\n",
      "fc1.bias torch.Size([120]) True\n",
      "fc2.weight torch.Size([84, 120]) True\n",
      "fc2.bias torch.Size([84]) True\n",
      "fc3.weight torch.Size([10, 84]) True\n",
      "fc3.bias torch.Size([10]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size(), param.requires_grad)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Running a foward pass\n",
    "\n",
    "To run a forward pass of your initial network over your testing set, simply run the following code. \n",
    "\n",
    "Note that `with torch.no_grad()` is used to avoid tracking for\n",
    "gradient during testing and then save some computation time\n",
    "(refer to https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 6, 5, 5], expected input[10000, 1, 28, 28] to have 6 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_torch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/hedi/D/notebook/jupyterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36mLeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m maxpool1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(conv1, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     18\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m  F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(maxpool1)) \u001b[38;5;66;03m# COMPLETE\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m maxpool2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m# COMPLETE\u001b[39;00m\n\u001b[1;32m     20\u001b[0m flattened \u001b[38;5;241m=\u001b[39m maxpool2\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_flat_features(maxpool2))\n\u001b[1;32m     21\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(flattened))  \u001b[38;5;66;03m# COMPLETE\u001b[39;00m\n",
      "File \u001b[0;32m/media/hedi/D/notebook/jupyterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/hedi/D/notebook/jupyterenv/lib/python3.8/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/hedi/D/notebook/jupyterenv/lib/python3.8/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 6, 5, 5], expected input[10000, 1, 28, 28] to have 6 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = net(x_test_torch) # equivalent to pred = net.forward(x_test_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3## Question 12\n",
    "\n",
    "Run the following and interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (y_test_torch \u001b[38;5;241m==\u001b[39m pred_labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "_, pred_labels = pred.max(1)\n",
    "print(100 * (y_test_torch == pred_labels).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "We will use (Mini-Batch) Stochastic Gradient Descent (SGD) with momentum, and cross-entropy as the loss.\n",
    "Complete the following function.\n",
    "\n",
    "For more details, refer to\n",
    "https://pytorch.org/docs/stable/nn.html and\n",
    "https://pytorch.org/docs/stable/optim.html.\n",
    "\n",
    "Note that PyTorch's `CrossEntropyLoss` is already the composition of a softmax activation with the standard cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T: number of epochs\n",
    "# B: minibatch size, \n",
    "# gamma: step size,\n",
    "# rho: momentum.\n",
    "def backprop_deep(x_train, y_train, net, T, B=100, gamma=.001, rho=.9):\n",
    "    N = x_train.size()[0] # Training set size\n",
    "    NB = # COMPLETE   # Number of minibatches\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=..., momentum=rho) # COMPLETE\n",
    "    \n",
    "    loss_values = []\n",
    "    for epoch in range(T):\n",
    "        running_loss = 0.0\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for k in range(NB):\n",
    "            # Extract k-th minibatch from xtrain and ltrain\n",
    "            minibatch_indices = shuffled_indices[B*k:min(B*(k+1), N)]\n",
    "            inputs = # COMPLETE\n",
    "            labels = # COMPLETE\n",
    "\n",
    "            # Initialize the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward propogation\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            # Error evaluation\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Back propogation\n",
    "            # COMPLETE\n",
    "\n",
    "            # Optimize step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute and print statistics\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()*inputs.size(0)\n",
    "  \n",
    "            \n",
    "        loss_values.append(running_loss/NB)\n",
    "  \n",
    "    plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "Run the function for 50 epochs, it may take several minutes. The number of epochs has to be adapted to reach convergence as most as possible.\n",
    "The loss per minibatch should decay (it may take some time). Explain the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop_deep(x_train_torch, y_train_torch, net, T=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 - Optional\n",
    "\n",
    "If you have a GPU, experiment the following and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device is not 'cpu':\n",
    "    net = LeNet().to(device)\n",
    "    xtrain = xtrain.to(device)\n",
    "    ltrain = ltrain.to(device)\n",
    "    t = time.time()\n",
    "    backprop_deep(xtrain, ltrain, net, T=50)\n",
    "    print(time.time() - t)\n",
    "    net = net.to('cpu')\n",
    "    xtrain_torch = xtrain_torch.to('cpu')\n",
    "    ltrain_torch = ltrain_torch.to('cpu')\n",
    "else:\n",
    "    print('Sorry no GPU')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "\n",
    "Analyse the results obtained by applying the network to the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5 - Comparison and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Compare all results and conclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*COMPLETE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
